# Arabic Synthetic Dataset Pipeline

A comprehensive pipeline for generating high-quality Arabic synthetic datasets using persona-based LLM generation with seed constraints and quality validation.

## ğŸ¯ Tasks

- **EXAMS**: Multi-subject MCQ questions (10,000 target)
- **Alghafa Sentiment**: Sentiment classification (positive:negative:neutral = 4:4:2, 10,000 target)  
- **Madinah QA Grammar**: Grammar error correction triplets (10,000 target)

## ğŸš€ Features

- **Persona-based Generation**: Role-playing prompts for authentic Arabic content
- **Seed Constraint System**: Uses â‰¤10 test samples as style guidance only (prevents data leakage)
- **Distribution Alignment**: Controlled answer distribution with quota scheduling
- **Quality Validation**: Fidelity, utility (TSTR), and privacy metrics
- **Data Augmentation**: Rule-based transformations and diversity filtering
- **CLI Interface**: Full pipeline orchestration with Typer

## ğŸ—ï¸ Architecture

```
src/arabic_synth/
â”œâ”€â”€ cli.py                     # ğŸ¯ Main CLI interface (Typer-based)
â”œâ”€â”€ data_prep/                 # ğŸ“Š DATA PREPARATION PHASE
â”‚   â”œâ”€â”€ exam_processor.py      # Combined sampling & CSV-to-JSONL conversion
â”‚   â””â”€â”€ personas_select.py     # Persona selection and filtering
â”œâ”€â”€ generators/                # ğŸš€ GENERATION PHASE
â”‚   â”œâ”€â”€ run.py                 # Core generation logic with seed constraints
â”‚   â””â”€â”€ persona_augment.py     # Persona-augmented generation utilities
â”œâ”€â”€ persona/                   # ğŸ‘¤ PERSONA PIPELINE
â”‚   â”œâ”€â”€ build_requests.py      # Build persona-augmented requests
â”‚   â”œâ”€â”€ send_requests.py       # Send requests to LLM APIs
â”‚   â””â”€â”€ templates_persona.py   # Persona-specific prompt templates
â”œâ”€â”€ postprocess/               # ğŸ§¹ POST-PROCESSING PHASE
â”‚   â””â”€â”€ clean.py               # Data cleaning, validation & deduplication
â”œâ”€â”€ evaluate/                  # ğŸ“Š EVALUATION PHASE
â”‚   â”œâ”€â”€ evaluate_style.py      # Style Guide Pipeline evaluation
â”‚   â””â”€â”€ evaluate_persona.py    # Persona-augmented quality assessment
â”œâ”€â”€ augment/                   # ğŸ”„ AUGMENTATION PHASE
â”‚   â””â”€â”€ augment.py             # Data augmentation and variant generation
â”œâ”€â”€ prompts/                   # ğŸ“ PROMPT TEMPLATES
â”‚   â””â”€â”€ templates.py           # LLM prompt templates (Style Guide)
â”œâ”€â”€ schemas/                   # ğŸ” VALIDATION SCHEMAS
â”‚   â”œâ”€â”€ exams.py               # Exam data structure validation
â”‚   â”œâ”€â”€ sentiment.py           # Sentiment analysis schemas
â”‚   â””â”€â”€ grammar.py             # Grammar correction schemas
â””â”€â”€ utils/                     # ğŸ› ï¸ CORE UTILITIES
    â”œâ”€â”€ llm.py                 # LLM API integration (OpenAI, etc.)
    â”œâ”€â”€ seed_manager.py        # Seed constraint system
    â”œâ”€â”€ quality_validator.py   # Quality metrics and validation
    â”œâ”€â”€ io.py                  # File I/O operations
    â””â”€â”€ anonymizer.py          # Data anonymization utilities
```

## âš™ï¸ Setup

```bash
# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install package
pip install -e .

# Set OpenAI API key
export OPENAI_API_KEY="your-api-key-here"
```

## ğŸ“– Workflows

The pipeline supports three main workflows for Arabic synthetic data generation:

### ğŸ¨ Style Guide Workflow
**Purpose**: Generate data following consistent style patterns from seed examples
1. **Seed Selection**: Extract/sample representative examples
2. **ğŸ¯ Style-Based Generation** âš ï¸ **CURRENT FOCUS**: Use `generators/run.py` with style templates (`prompts/templates.py`)
   - **Iterative Prompt Tuning**: Manual output checks and template refinement required
   - **Template Location**: `./src/arabic_synth/prompts/templates.py`
   - **Process**: Generate â†’ Review â†’ Tune â†’ Repeat until quality targets met
3. **Post-Processing**: Clean, validate, and evaluate quality

```bash
# Quick start - Style Guide Pipeline
arabic-synth sample-and-convert --input-file data/test-*.csv --output-file outputs/seeds.jsonl --n 10 --mode stratified

# âš ï¸ ITERATIVE TUNING REQUIRED: Check outputs and refine prompts/templates.py
arabic-synth generate exams --seed-file outputs/seeds.jsonl --model openai:gpt-4o --num-samples 200

arabic-synth clean exams --in-path outputs/exams_raw.jsonl --out-path outputs/exams_clean.jsonl
arabic-synth evaluate-style exams --in-path outputs/exams_clean.jsonl
```

### ğŸ‘¤ Persona Enhanced Workflow  
**Purpose**: Generate diverse data using persona-based perspectives
1. **Seed Selection**: Prepare base examples for persona augmentation
2. **ğŸ¯ Persona Requests** âš ï¸ **CURRENT FOCUS**: Build requests with persona templates (`persona/templates_persona.py`)
   - **Iterative Prompt Tuning**: Manual output checks and template refinement required
   - **Template Location**: `./src/arabic_synth/persona/templates_persona.py`
   - **Process**: Generate â†’ Review â†’ Tune â†’ Repeat until persona consistency achieved
3. **Post-Processing**: Quality assessment and evaluation

```bash
# Quick start - Persona Pipeline
arabic-synth sample-and-convert --input-file data/test-*.csv --output-file outputs/seeds.jsonl --n 20
arabic-synth select-personas --input-file data/personas/personas_all.jsonl --output-file outputs/personas.jsonl --n 200

# âš ï¸ ITERATIVE TUNING REQUIRED: Check outputs and refine persona/templates_persona.py
arabic-synth build-persona-requests --exams-path outputs/seeds.jsonl --personas-path outputs/personas.jsonl
arabic-synth send-persona-requests --model openai:gpt-4o

arabic-synth evaluate-persona --input-file outputs/exams_pers_raw.jsonl
```

### ğŸ”„ Combined Style-Persona Workflow
**Purpose**: Best of both worlds - style consistency with persona diversity
1. **Stratified Sampling**: Extract balanced seeds from original test data
2. **Style Guide Generation**: Create styled examples using seed constraints  
3. **Persona Selection**: Choose diverse personas for augmentation
4. **Combined Generation**: Apply personas to styled seeds for maximum diversity

```bash
# Quick start - Combined Workflow (RECOMMENDED)
arabic-synth style-persona-workflow \
  --input-csv data/test-00000-of-00001.arabic.csv \
  --personas-path data/personas/selected_200.jsonl \
  --n-seeds 20 \
  --n-styled 100 \
  --per-item-personas 5 \
  --model openai:gpt-4o
```

**ğŸ“‹ Detailed Guides**: 
- Style Guide: `StyleGuide_PIPELINE_DETAILED.md`
- Persona Pipeline: `Persona_PIPELINE_DETAILED.md`  
- Combined Workflow: `STYLE_PERSONA_WORKFLOW_GUIDE.md`

---

## âš ï¸ **CURRENT DEVELOPMENT FOCUS**

The following components require **active iterative prompt tuning** through manual output review:

### ğŸ¯ **Priority Tasks:**
1. **Style Guide Templates** (`src/arabic_synth/prompts/templates.py`)
   - Fine-tune `EXAMS_TEACHER_PROMPT` for better Arabic naturalness
   - Optimize style consistency and content diversity balance
   - Target: 95%+ quality scores in style evaluation

2. **Persona Templates** (`src/arabic_synth/persona/templates_persona.py`) 
   - Refine `ARABIC_EXAM_REWRITE_V1` for authentic persona perspectives
   - Ensure persona consistency while maintaining content accuracy
   - Target: Diverse yet coherent persona-based variations

### ğŸ”„ **Iterative Process:**
```
Generate Sample â†’ Manual Review â†’ Identify Issues â†’ Tune Prompts â†’ Repeat
```

**Evaluation Commands for Tuning:**
```bash
# Test style guide outputs
arabic-synth evaluate-style exams --in-path outputs/exams_clean.jsonl

# Test persona outputs  
arabic-synth evaluate-persona --input-file outputs/exams_pers_raw.jsonl
```

## ğŸ”’ Seed Constraint System

The pipeline uses a sophisticated seed constraint system to prevent data leakage:

- **Max Seeds**: â‰¤10 test samples allowed
- **Style Only**: Seeds provide style guidance, not content replication
- **Diversity Check**: Ensures seed variety across subjects
- **Similarity Validation**: Prevents generated content from being too similar to seeds
- **Audit Trail**: Full logging of seed usage and constraints

## ğŸ“Š Quality Metrics

### Fidelity
- Length distribution comparison (mean, std dev)
- Answer balance (L1 distance from target)
- Vocabulary diversity (Type-Token Ratio)
- Content overlap analysis (Jaccard similarity)

### Utility  
- **TSTR (Train on Synthetic, Test on Real)**: Trains classifier on synthetic data, tests on real data
- Performance comparison metrics

### Privacy
- Re-identification risk assessment
- Token overlap analysis
- Differential privacy considerations

## ğŸ›ï¸ Advanced Controls

- **Temperature & Top-p**: Control generation diversity
- **Answer Distribution**: Target specific answer letter ratios
- **TTR Filtering**: Remove low-diversity samples (default threshold: 0.18)
- **Batch Processing**: Configurable batch sizes for large-scale generation

## ğŸ“ Project Structure

```
Synthetic Data/
â”œâ”€â”€ src/arabic_synth/          # Core package
â”œâ”€â”€ data/seeds/                # Seed data (â‰¤10 samples)
â”œâ”€â”€ outputs/                   # Generated datasets
â”œâ”€â”€ test-00000-of-00001.arabic.csv  # Original test set
â”œâ”€â”€ pyproject.toml            # Package configuration
â”œâ”€â”€ README.md                 # This file
â””â”€â”€ SEED_CONSTRAINTS.md       # Detailed constraint documentation
```

## ğŸ”„ Pipeline Flow

1. **Seed Loading**: Extract style guidance from â‰¤10 test samples
2. **Generation**: LLM generation with persona prompts and distribution control
3. **Cleaning**: Schema validation, deduplication, quality filtering
4. **Evaluation**: Comprehensive quality assessment
5. **Export**: Format conversion with metadata

## ğŸš¦ Scaling Strategy

- **Pilot**: 100-200 samples for validation
- **Small Scale**: 1K samples for quality assessment  
- **Full Scale**: 10K samples with monitoring
- **Continuous**: Quality checks and retraining

## ğŸ› ï¸ Development

```bash
# Install development dependencies
pip install -e ".[dev]"

# Run tests
pytest

# Format code
black src/
```

## ğŸ“ Notes

- **Model Selection**: Currently supports OpenAI models via `openai:MODEL_NAME`
- **Mock Mode**: Use `--model mock` for testing without API calls
- **Seed Diversity**: Ensure seeds cover different subjects and difficulty levels
- **Quality Thresholds**: Adjust TTR and similarity thresholds as needed

## ğŸ¤ Contributing

1. Follow the seed constraint system
2. Maintain quality metrics
3. Test with mock mode first
4. Document any prompt or parameter changes 